{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf3f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, re\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cd5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape GP data\n",
    "def scrape_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    gp_blocks = soup.find_all('div', {'class': 'results__details'})\n",
    "    gp_data = []\n",
    "    for block in gp_blocks:\n",
    "        name = block.find('h2', {'id': lambda x: x and x.startswith('orgname_')}).text.strip()\n",
    "        address = block.find('p', {'id': lambda x: x and x.startswith('address_')}).text.strip()\n",
    "        phone = block.find('p', {'id': lambda x: x and x.startswith('phone_')}).text.strip()\n",
    "        tags = block.find_all('strong', {'id': lambda x: x and x.startswith('result_item_')})\n",
    "        tags_text = [tag.text.strip() for tag in tags]\n",
    "        gp_data.append({\n",
    "            'Name': name,\n",
    "            'Address': address,\n",
    "            'Phone': phone,\n",
    "            'Accepting New Patients': 'Accepting new patients' in tags_text,\n",
    "            'Accepts Out of Area Registrations': 'Accepts out of area registrations' in tags_text,\n",
    "            'Online Registration Available': 'Online registration available' in tags_text\n",
    "        })\n",
    "    return gp_data\n",
    "\n",
    "# Function to scrape initial GP URLs\n",
    "def scrape_gp_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    gp_links = []\n",
    "    for link in soup.find_all('a', {'class': 'nhsapp-open-in-webview'}):\n",
    "        href = link.get('href')\n",
    "        if not (href.startswith('javascript') or href.startswith('#')):\n",
    "            gp_links.append(href)\n",
    "    return gp_links\n",
    "\n",
    "# Scrape Reviews Function\n",
    "def scrape_reviews(gp_links):\n",
    "    reviews_data = []\n",
    "    ratings_data = []\n",
    "    for gp_url in gp_links:\n",
    "        review_url = f\"{gp_url}/ratings-and-reviews\"\n",
    "        try:\n",
    "            response = requests.get(review_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            gp_name = soup.find('h1').text.strip().split('\\n')[0].strip()\n",
    "\n",
    "            review_blocks = soup.find_all('div', {'class': 'org-review'})\n",
    "            gp_ratings = []\n",
    "            for block in review_blocks:\n",
    "                review_text_block = block.find('p', class_='comment-text')\n",
    "                if review_text_block:\n",
    "                    review_text = review_text_block.get_text(strip=True)\n",
    "                    reviews_data.append({'GPName': gp_name, 'ReviewText': review_text})\n",
    "                \n",
    "                # Extracting rating\n",
    "                rating_text = block.find('p', {'id': re.compile(r'star-rating-.*')}).text.strip()\n",
    "                if rating_text:\n",
    "                    rating_value = float(rating_text.split(' ')[1])\n",
    "                    gp_ratings.append(rating_value)\n",
    "            \n",
    "            # Calculate average rating if ratings were found\n",
    "            if gp_ratings:\n",
    "                average_rating = sum(gp_ratings) / len(gp_ratings)\n",
    "                ratings_data.append({'GPName': gp_name, 'AverageRating': average_rating})\n",
    "                \n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Could not fetch reviews for {gp_url}: {e}\")\n",
    "    \n",
    "    return reviews_data, ratings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e105c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create latitude and longitude from addresses using postcode\n",
    "def geocode_address(address):\n",
    "    geolocator = Nominatim(user_agent=\"GP_Finder_App\")\n",
    "    # Define the regex pattern for UK postcodes\n",
    "    postcode_pattern = r'[A-Z]{1,2}[0-9R][0-9A-Z]? ?[0-9][A-Z]{2}'\n",
    "    # Search for the postcode within the address using the regex pattern\n",
    "    postcode_search = re.search(postcode_pattern, address)\n",
    "    # If a postcode is found, use it for geocoding\n",
    "    if postcode_search:\n",
    "        postcode = postcode_search.group()\n",
    "        location = geolocator.geocode(postcode)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d486f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Export Data to CSV\n",
    "def export_to_csv(data, filename, folder_path):\n",
    "    # Construct the full file path\n",
    "    full_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "    \n",
    "    # Convert data to DataFrame and save as CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(full_path, index=False)\n",
    "    print(f\"Data exported to {full_path} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00ce06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a postcode: sw129lq\n",
      "Data exported to C:/Users/rober/DA/PersonalProjects/GP-Finder/data\\gp_info.csv successfully.\n",
      "Data exported to C:/Users/rober/DA/PersonalProjects/GP-Finder/data\\gp_reviews.csv successfully.\n",
      "Data exported to C:/Users/rober/DA/PersonalProjects/GP-Finder/data\\gp_ratings.csv successfully.\n",
      "Data scraping and export completed.\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the folder path for data storage\n",
    "    folder_path = \"C:/Users/rober/DA/PersonalProjects/GP-Finder/data\"\n",
    "\n",
    "    # Ask the user for a postcode\n",
    "    postcode = input(\"Enter a postcode: \")\n",
    "    \n",
    "    # Update the URL with the user's postcode\n",
    "    url = f\"https://www.nhs.uk/service-search/find-a-gp/results/{postcode}\"\n",
    "    \n",
    "    # Scrape data\n",
    "    gp_data = scrape_data(url)\n",
    "    gp_links = scrape_gp_links(url)\n",
    "    reviews_data, ratings_data = scrape_reviews(gp_links)\n",
    "\n",
    "    # Geocode the addresses in the GP data\n",
    "    for gp in gp_data:\n",
    "        latitude, longitude = geocode_address(gp['Address'])\n",
    "        gp['Latitude'] = latitude\n",
    "        gp['Longitude'] = longitude\n",
    "\n",
    "    # Export GP data to CSV\n",
    "    export_to_csv(gp_data, 'gp_info.csv', folder_path)\n",
    "\n",
    "    # Export Reviews data to CSV\n",
    "    export_to_csv(reviews_data, 'gp_reviews.csv', folder_path)\n",
    "\n",
    "    # Export Ratings data to CSV\n",
    "    export_to_csv(ratings_data, 'gp_ratings.csv', folder_path)\n",
    "\n",
    "    print(\"Data scraping and export completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
